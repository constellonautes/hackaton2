{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task4_semihardNegetive.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GOWnJuCBno7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "40f0fc2e-397a-4131-a290-7a54ca6ea8f7"
      },
      "source": [
        "!git clone https://github.com/jjmachan/DeepHash\n",
        "import DeepHash.datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DeepHash'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/27)\u001b[K\rremote: Counting objects:   7% (2/27)\u001b[K\rremote: Counting objects:  11% (3/27)\u001b[K\rremote: Counting objects:  14% (4/27)\u001b[K\rremote: Counting objects:  18% (5/27)\u001b[K\rremote: Counting objects:  22% (6/27)\u001b[K\rremote: Counting objects:  25% (7/27)\u001b[K\rremote: Counting objects:  29% (8/27)\u001b[K\rremote: Counting objects:  33% (9/27)\u001b[K\rremote: Counting objects:  37% (10/27)\u001b[K\rremote: Counting objects:  40% (11/27)\u001b[K\rremote: Counting objects:  44% (12/27)\u001b[K\rremote: Counting objects:  48% (13/27)\u001b[K\rremote: Counting objects:  51% (14/27)\u001b[K\rremote: Counting objects:  55% (15/27)\u001b[K\rremote: Counting objects:  59% (16/27)\u001b[K\rremote: Counting objects:  62% (17/27)\u001b[K\rremote: Counting objects:  66% (18/27)\u001b[K\rremote: Counting objects:  70% (19/27)\u001b[K\rremote: Counting objects:  74% (20/27)\u001b[K\rremote: Counting objects:  77% (21/27)\u001b[K\rremote: Counting objects:  81% (22/27)\u001b[K\rremote: Counting objects:  85% (23/27)\u001b[K\rremote: Counting objects:  88% (24/27)\u001b[K\rremote: Counting objects:  92% (25/27)\u001b[K\rremote: Counting objects:  96% (26/27)\u001b[K\rremote: Counting objects: 100% (27/27)\u001b[K\rremote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects:   4% (1/21)\u001b[K\rremote: Compressing objects:   9% (2/21)\u001b[K\rremote: Compressing objects:  14% (3/21)\u001b[K\rremote: Compressing objects:  19% (4/21)\u001b[K\rremote: Compressing objects:  23% (5/21)\u001b[K\rremote: Compressing objects:  28% (6/21)\u001b[K\rremote: Compressing objects:  33% (7/21)\u001b[K\rremote: Compressing objects:  38% (8/21)\u001b[K\rremote: Compressing objects:  42% (9/21)\u001b[K\rremote: Compressing objects:  47% (10/21)\u001b[K\rremote: Compressing objects:  52% (11/21)\u001b[K\rremote: Compressing objects:  57% (12/21)\u001b[K\rremote: Compressing objects:  61% (13/21)\u001b[K\rremote: Compressing objects:  66% (14/21)\u001b[K\rremote: Compressing objects:  71% (15/21)\u001b[K\rremote: Compressing objects:  76% (16/21)\u001b[K\rremote: Compressing objects:  80% (17/21)\u001b[K\rremote: Compressing objects:  85% (18/21)\u001b[K\rremote: Compressing objects:  90% (19/21)\u001b[K\rremote: Compressing objects:  95% (20/21)\u001b[K\rremote: Compressing objects: 100% (21/21)\u001b[K\rremote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "Receiving objects:   0% (1/170)   \rReceiving objects:   1% (2/170)   \rReceiving objects:   2% (4/170)   \rReceiving objects:   3% (6/170)   \rReceiving objects:   4% (7/170)   \rReceiving objects:   5% (9/170)   \rReceiving objects:   6% (11/170)   \rReceiving objects:   7% (12/170)   \rReceiving objects:   8% (14/170)   \rReceiving objects:   9% (16/170)   \rReceiving objects:  10% (17/170)   \rReceiving objects:  11% (19/170)   \rReceiving objects:  12% (21/170)   \rReceiving objects:  13% (23/170)   \rReceiving objects:  14% (24/170)   \rReceiving objects:  15% (26/170)   \rReceiving objects:  16% (28/170)   \rReceiving objects:  17% (29/170)   \rReceiving objects:  18% (31/170)   \rReceiving objects:  19% (33/170)   \rReceiving objects:  20% (34/170)   \rReceiving objects:  21% (36/170)   \rReceiving objects:  22% (38/170)   \rReceiving objects:  23% (40/170)   \rReceiving objects:  24% (41/170)   \rReceiving objects:  25% (43/170)   \rReceiving objects:  26% (45/170)   \rReceiving objects:  27% (46/170)   \rReceiving objects:  28% (48/170)   \rReceiving objects:  29% (50/170)   \rReceiving objects:  30% (51/170)   \rReceiving objects:  31% (53/170)   \rReceiving objects:  32% (55/170)   \rReceiving objects:  33% (57/170)   \rReceiving objects:  34% (58/170)   \rReceiving objects:  35% (60/170)   \rReceiving objects:  36% (62/170)   \rReceiving objects:  37% (63/170)   \rReceiving objects:  38% (65/170)   \rReceiving objects:  39% (67/170)   \rReceiving objects:  40% (68/170)   \rReceiving objects:  41% (70/170)   \rReceiving objects:  42% (72/170)   \rReceiving objects:  43% (74/170)   \rReceiving objects:  44% (75/170)   \rReceiving objects:  45% (77/170)   \rReceiving objects:  46% (79/170)   \rReceiving objects:  47% (80/170)   \rReceiving objects:  48% (82/170)   \rReceiving objects:  49% (84/170)   \rReceiving objects:  50% (85/170)   \rReceiving objects:  51% (87/170)   \rReceiving objects:  52% (89/170)   \rReceiving objects:  53% (91/170)   \rReceiving objects:  54% (92/170)   \rReceiving objects:  55% (94/170)   \rReceiving objects:  56% (96/170)   \rReceiving objects:  57% (97/170)   \rReceiving objects:  58% (99/170)   \rReceiving objects:  59% (101/170)   \rReceiving objects:  60% (102/170)   \rReceiving objects:  61% (104/170)   \rReceiving objects:  62% (106/170)   \rReceiving objects:  63% (108/170)   \rReceiving objects:  64% (109/170)   \rReceiving objects:  65% (111/170)   \rReceiving objects:  66% (113/170)   \rReceiving objects:  67% (114/170)   \rReceiving objects:  68% (116/170)   \rReceiving objects:  69% (118/170)   \rReceiving objects:  70% (119/170)   \rReceiving objects:  71% (121/170)   \rReceiving objects:  72% (123/170)   \rReceiving objects:  73% (125/170)   \rReceiving objects:  74% (126/170)   \rReceiving objects:  75% (128/170)   \rReceiving objects:  76% (130/170)   \rReceiving objects:  77% (131/170)   \rReceiving objects:  78% (133/170)   \rReceiving objects:  79% (135/170)   \rReceiving objects:  80% (136/170)   \rReceiving objects:  81% (138/170)   \rReceiving objects:  82% (140/170)   \rReceiving objects:  83% (142/170)   \rReceiving objects:  84% (143/170)   \rReceiving objects:  85% (145/170)   \rReceiving objects:  86% (147/170)   \rReceiving objects:  87% (148/170)   \rReceiving objects:  88% (150/170)   \rReceiving objects:  89% (152/170)   \rReceiving objects:  90% (153/170)   \rReceiving objects:  91% (155/170)   \rReceiving objects:  92% (157/170)   \rReceiving objects:  93% (159/170)   \rReceiving objects:  94% (160/170)   \rReceiving objects:  95% (162/170)   \rReceiving objects:  96% (164/170)   \rReceiving objects:  97% (165/170)   \rremote: Total 170 (delta 10), reused 18 (delta 6), pack-reused 143\u001b[K\n",
            "Receiving objects:  98% (167/170)   \rReceiving objects:  99% (169/170)   \rReceiving objects: 100% (170/170)   \rReceiving objects: 100% (170/170), 12.98 MiB | 32.03 MiB/s, done.\n",
            "Resolving deltas:   0% (0/52)   \rResolving deltas:   1% (1/52)   \rResolving deltas:   7% (4/52)   \rResolving deltas:  15% (8/52)   \rResolving deltas:  25% (13/52)   \rResolving deltas:  28% (15/52)   \rResolving deltas:  42% (22/52)   \rResolving deltas:  46% (24/52)   \rResolving deltas:  67% (35/52)   \rResolving deltas:  75% (39/52)   \rResolving deltas:  76% (40/52)   \rResolving deltas:  88% (46/52)   \rResolving deltas:  90% (47/52)   \rResolving deltas:  92% (48/52)   \rResolving deltas:  96% (50/52)   \rResolving deltas: 100% (52/52)   \rResolving deltas: 100% (52/52), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32ogziDutEaR",
        "colab_type": "text"
      },
      "source": [
        "# Training Triplet Network using SemiHardNegetive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9u5OIoawwId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from DeepHash.trainer import fit\n",
        "cuda = torch.cuda.is_available()\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as pl\n",
        "\n",
        "from DeepHash.utils import freeze_model, list_trainable, del_last_layers, save, load, create_embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD93VGcow0WB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "b1644755-25ac-4b2c-b217-cb09d08d1e01"
      },
      "source": [
        "batch_size = 64\n",
        "num_workers = 1\n",
        "\n",
        "#define transforms\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# load imagenet\n",
        "image_dataset = {\n",
        "        'train' :datasets.CIFAR10('./', train=True, download=True, transform=data_transforms['train']),\n",
        "        'test' : datasets.CIFAR10('./', train=False, download=True, transform=data_transforms['train']) \n",
        "}\n",
        "\n",
        "# Create the dataloaders\n",
        "data_loader = {\n",
        "    'train': torch.utils.data.DataLoader(image_dataset['train'], batch_size=batch_size, shuffle=True, num_workers=num_workers),\n",
        "    'test': torch.utils.data.DataLoader(image_dataset['test'], batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/170498071 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:01, 89374698.22it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to ./\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j7_JBDlw6zg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6d79fd12-a546-4669-e81b-93744d4f88d5"
      },
      "source": [
        "# An identity layer to pass the fc layer in resnet\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x\n",
        "  \n",
        "# Define model\n",
        "resnet18  = models.resnet18(pretrained=True)\n",
        "resnet18.fc = Identity()\n",
        "\n",
        "# Freeze all the parameters in the model\n",
        "def freeze_model(model):\n",
        "  for params in model.parameters():\n",
        "    params.requires_grad=False\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 120MB/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xnXFaWMlxnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up data loaders\n",
        "from DeepHash.datasets import BalancedBatchSamplerCifar\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "train_batch_sampler = BalancedBatchSamplerCifar(image_dataset['train'].targets, n_classes=10, n_samples=25)\n",
        "test_batch_sampler = BalancedBatchSamplerCifar(image_dataset['test'].targets, n_classes=10, n_samples=25)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg0n98HUJMp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "online_train_loader = torch.utils.data.DataLoader(image_dataset['train'], batch_sampler=train_batch_sampler, **kwargs)\n",
        "online_test_loader = torch.utils.data.DataLoader(image_dataset['test'], batch_sampler=test_batch_sampler, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR3nVDnvMdLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loader = iter(online_train_loader)\n",
        "images, labels = next(loader)\n",
        "print(images.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InBCTR7sLvrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up the network and training parameters\n",
        "from DeepHash.networks import EmbeddingNet\n",
        "from DeepHash.losses import OnlineTripletLoss\n",
        "from DeepHash.utils import AllTripletSelector,HardestNegativeTripletSelector, RandomNegativeTripletSelector, SemihardNegativeTripletSelector # Strategies for selecting triplets within a minibatch\n",
        "from DeepHash.metrics import AverageNonzeroTripletsMetric\n",
        "\n",
        "margin = 1.\n",
        "embedding_net = resnet18\n",
        "model = embedding_net\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "loss_fn = OnlineTripletLoss(margin, SemihardNegativeTripletSelector(margin))\n",
        "lr = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
        "n_epochs = 20\n",
        "log_interval = 15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUkHpGiVl1JX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "67a47626-1e63-47c0-81e6-0e53e12b92ac"
      },
      "source": [
        "fit(online_train_loader, online_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[AverageNonzeroTripletsMetric()])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train: [0/50000 (0%)]\tLoss: 0.512598\tAverage nonzero triplets: 1471.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-aba445036be0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monline_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monline_test_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAverageNonzeroTripletsMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/DeepHash/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics, start_epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Train stage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Epoch: {}/{}. Train set: Average loss: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DeepHash/trainer.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mloss_inputs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mloss_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mloss_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DeepHash/losses.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embeddings, target)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mtriplets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriplet_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_triplets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/DeepHash/utils.py\u001b[0m in \u001b[0;36mget_triplets\u001b[0;34m(self, embeddings, labels)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mdistance_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mdistance_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzxfPC3avk-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model, './triplet_resnet18_SemihardNegetiveTripletSelector.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtqvO2WPKUnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = OnlineTripletLoss(margin, HardestNegativeTripletSelector(margin))\n",
        "unfreeze_model(model)\n",
        "fit(online_train_loader, online_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[AverageNonzeroTripletsMetric()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD21WqXCHlqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model, './triplet_vgg20_2.mdl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddYXN5naIdXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kodDsXoMwF6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save to google drive \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "!cp *.mdl /gdrive/My\\ Drive/tooploox"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP3tTynrwS_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "!cp /gdrive/My\\ Drive/tooploox/*.mdl ./ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj5dM0U2v4f9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_test = torch.load('./triplet_vgg19.mdl')\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGjNaMPZyvWA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c884b499-1893-40bc-d272-e49582d5493b"
      },
      "source": [
        "model_emb = model\n",
        "freeze_model(model_emb)\n",
        "print(model_emb)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Identity()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8_GyOjMylBx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e2a2b6ae-cbc0-43d3-ca99-efdaeabcc35a"
      },
      "source": [
        "features, targets = create_embeddings(model_emb, data_loader, 512)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "0\n",
            "(64, 512)\n",
            "error occured: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Miqi61eqAIvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save the computed embeddings\n",
        "save(features, targets, 'resnet18_semihardNegetive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6waqp5oATjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save to google drive \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "!cp *embs /gdrive/My\\ Drive/tooploox"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP891c-sYuQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls /gdrive/My\\ Drive/tooploox"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzrALFxou2zM",
        "colab_type": "text"
      },
      "source": [
        "# Classification loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpDtuaMfAUms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "k_range = range(25,30)\n",
        "\n",
        "def search_knn_accuracies(k_range, features, targets):\n",
        "  acc = []\n",
        "  for k in k_range:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(features['train'], targets['train'])\n",
        "    #print('finished fitting')\n",
        "    predict = knn.predict(features['test'][:300,:])\n",
        "    #print('predicted')\n",
        "    score = metrics.accuracy_score(targets['test'][:300], predict)\n",
        "    print('K value: %d, accuracy: %0.7f' %(k, score))\n",
        "    acc.append(score)\n",
        "  print('Mean accuracy ',sum(acc)/len(acc))\n",
        "\n",
        "# the best score was obtained when k = 20:24\n",
        "search_knn_accuracies(k_range, features, targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEOVHcB7u697",
        "colab_type": "text"
      },
      "source": [
        "# t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFy1JSKSAZU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as PathEffects\n",
        "from sklearn.manifold import TSNE\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "sns.set_palette('muted')\n",
        "sns.set_context(\"notebook\", font_scale=1.5,\n",
        "                rc={\"lines.linewidth\": 2.5})\n",
        "RS = 123\n",
        "# need to create a subset of the data, too much time to process otherwise\n",
        "x_subset = features['train'][:5000]\n",
        "y_subset = targets['train'][:5000]\n",
        "\n",
        "print(np.unique(y_subset))\n",
        "labels = {\n",
        "     0: 'airplane',  \n",
        "     1: 'automobile',\n",
        "     2: 'bird',\n",
        "     3: 'cat',\n",
        "     4: 'deer',\n",
        "     5: 'dog',\n",
        "     6: 'frog',\n",
        "     7: 'horse',\n",
        "     8: 'ship',\n",
        "     9: 'truck',\n",
        "}\n",
        "# Utility function to visualize the outputs of PCA and t-SNE\n",
        "\n",
        "def fashion_scatter(x, colors):\n",
        "    # choose a color palette with seaborn.\n",
        "    num_classes = len(np.unique(colors))\n",
        "    palette = np.array(sns.color_palette(\"hls\", num_classes))\n",
        "\n",
        "    # create a scatter plot.\n",
        "    f = plt.figure(figsize=(8, 8))\n",
        "    ax = plt.subplot(aspect='equal')\n",
        "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)])\n",
        "    plt.xlim(-25, 25)\n",
        "    plt.ylim(-25, 25)\n",
        "    ax.axis('off')\n",
        "    ax.axis('tight')\n",
        "\n",
        "    # add the labels for each digit corresponding to the label\n",
        "    txts = []\n",
        "\n",
        "    for i in range(num_classes):\n",
        "\n",
        "        # Position of each label at median of data points.\n",
        "\n",
        "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
        "        txt = ax.text(xtext, ytext, str(labels[i]), fontsize=24)\n",
        "        txt.set_path_effects([\n",
        "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
        "            PathEffects.Normal()])\n",
        "        txts.append(txt)\n",
        "\n",
        "    return f, ax, sc, txts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD-BsELyA6BR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# do pca before passing to tsne to reduce noice and fast performance\n",
        "time_start = time.time()\n",
        "\n",
        "pca_50 = PCA(n_components=50)\n",
        "pca_result_50 = pca_50.fit_transform(x_subset)\n",
        "\n",
        "print('PCA with 50 components done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "\n",
        "print('Cumulative variance explained by 50 principal components: {}'.format(np.sum(pca_50.explained_variance_ratio_)))\n",
        "# perform tsne on 50 components\n",
        "time_start = time.time()\n",
        "\n",
        "\n",
        "fashion_pca_tsne = TSNE(random_state=RS).fit_transform(pca_result_50)\n",
        "\n",
        "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2HvhLmEA-ah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fashion_scatter(fashion_pca_tsne, y_subset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqfk3460dGYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3T9eTgFHje3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}