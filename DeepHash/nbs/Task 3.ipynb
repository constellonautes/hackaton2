{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GOWnJuCBno7"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/jjmachan/DeepHash.git\n",
    "import DeepHash.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M62tQcAHntn2"
   },
   "source": [
    "# Task 3\n",
    "## Training Using the Triplet Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y9u5OIoawwId"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "\n",
    "from DeepHash.trainer import fit\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from DeepHash.utils import freeze_model, list_trainable, del_last_layers, save, load, create_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZD93VGcow0WB"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 1\n",
    "\n",
    "#define transforms\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# load imagenet\n",
    "image_dataset = {\n",
    "        'train' :datasets.CIFAR10('./', train=True, download=True, transform=data_transforms['train']),\n",
    "        'test' : datasets.CIFAR10('./', train=False, download=True, transform=data_transforms['train']) \n",
    "}\n",
    "\n",
    "# Create the dataloaders\n",
    "data_loader = {\n",
    "    'train': torch.utils.data.DataLoader(image_dataset['train'], batch_size=batch_size, shuffle=True, num_workers=num_workers),\n",
    "    'test': torch.utils.data.DataLoader(image_dataset['test'], batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-j7_JBDlw6zg"
   },
   "outputs": [],
   "source": [
    "# An identity layer to pass the fc layer in resnet\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "  \n",
    "# Define model\n",
    "resnet18  = models.resnet18(pretrained=True)\n",
    "resnet18.fc = Identity()\n",
    "\n",
    "# Freeze all the parameters in the model\n",
    "def freeze_model(model):\n",
    "  for params in model.parameters():\n",
    "    params.requires_grad=False\n",
    "\n",
    "# freeze\n",
    "#freeze_model(vgg19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bnvccdqb6c0j"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(triplet_train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images[0].shape)\n",
    "# show images\n",
    "imshow(images[0][0])\n",
    "# print labels\n",
    "#print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8xnXFaWMlxnP"
   },
   "outputs": [],
   "source": [
    "# Set up data loaders\n",
    "from DeepHash.datasets import TripletCifar1\n",
    "from triplet.datasets import TripletMNIST\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "triplet_train_dataset = TripletCifar1(image_dataset['train']) # Returns triplets of images\n",
    "triplet_test_dataset = TripletCifar1(image_dataset['test'])\n",
    "batch_size = 32\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "triplet_train_loader = torch.utils.data.DataLoader(triplet_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "triplet_test_loader = torch.utils.data.DataLoader(triplet_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "# Set up the network and training parameters\n",
    "from triplet.networks import EmbeddingNet, TripletNet\n",
    "from triplet.losses import TripletLoss\n",
    "\n",
    "margin = 2.\n",
    "embedding_net = resnet18\n",
    "model = TripletNet(embedding_net)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = TripletLoss(margin)\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 10\n",
    "log_interval = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kUkHpGiVl1JX"
   },
   "outputs": [],
   "source": [
    "fit(triplet_train_loader, triplet_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzxfPC3avk-D"
   },
   "outputs": [],
   "source": [
    "torch.save(model, './triplet_resnet18_2.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DtqvO2WPKUnG"
   },
   "outputs": [],
   "source": [
    "fit(triplet_train_loader, triplet_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AD21WqXCHlqc"
   },
   "outputs": [],
   "source": [
    "torch.save(model, './triplet_vgg20_3.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddYXN5naIdXp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kodDsXoMwF6-"
   },
   "outputs": [],
   "source": [
    "# save to google drive \n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "\n",
    "!cp *.mdl /gdrive/My\\ Drive/tooploox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kP3tTynrwS_I"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "!cp /gdrive/My\\ Drive/tooploox/*.mdl ./ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hj5dM0U2v4f9"
   },
   "outputs": [],
   "source": [
    "model_test = torch.load('./triplet_resnet18_2.mdl')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOw-fCVY_sGx"
   },
   "outputs": [],
   "source": [
    "# Freeze all the parameters in the model\n",
    "def freeze_model(model):\n",
    "  for params in model.parameters():\n",
    "    params.requires_grad=False\n",
    "\n",
    "# check if all the parameters have been freezed\n",
    "def list_trainable(model):\n",
    "  for params in model.parameters():\n",
    "    print(params.requires_grad)\n",
    "  \n",
    "# delete the last layers\n",
    "def del_last_layers(model_class, num_layers):\n",
    "  model_class = nn.Sequential(*list(model_class.children())[:-num_layers])\n",
    "  return model_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6-64Pswx7GH"
   },
   "outputs": [],
   "source": [
    "def create_embeddings(model, embedding_size):\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  print(device)\n",
    "\n",
    "  features = {}\n",
    "  targets = {}\n",
    "  model.to(device)\n",
    "  features['train'] = np.empty([0, embedding_size])\n",
    "  targets['train'] = np.empty([0, ])\n",
    "\n",
    "  features['test'] = np.empty([0, embedding_size])\n",
    "  targets['test'] = np.empty([0,])\n",
    "\n",
    "  for i, (images,target) in enumerate(data_loader['train']):\n",
    "    images = images.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    try:\n",
    "      output = model(images).cpu().numpy()\n",
    "      features['train'] = np.append(features['train'],output, axis=0)\n",
    "      targets['train'] = np.append(targets['train'],target.cpu(), axis=0)\n",
    "    except:\n",
    "      print(output.shape)\n",
    "      print('error occured: ')\n",
    "      return (None, None)\n",
    "      \n",
    "    if i%100 == 0:\n",
    "      print(i)\n",
    "\n",
    "  for i, (images,target) in enumerate(data_loader['test']):\n",
    "    images = images.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    output = model(images).cpu().numpy()\n",
    "    features['test'] = np.append(features['test'],output, axis=0)\n",
    "    targets['test'] = np.append(targets['test'],target.cpu(), axis=0)\n",
    "\n",
    "    if i%100 == 0:\n",
    "      print(i)\n",
    "  return (features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGjNaMPZyvWA"
   },
   "outputs": [],
   "source": [
    "model_emb = model.embedding_net\n",
    "freeze_model(model_emb)\n",
    "print(model_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8_GyOjMylBx"
   },
   "outputs": [],
   "source": [
    "from DeepHash.utils import create_embeddings\n",
    "features, targets = create_embeddings(model_emb, data_loader, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Miqi61eqAIvx"
   },
   "outputs": [],
   "source": [
    "# save the computed embeddings\n",
    "save(features, targets, 'resnet18_triplet_margin2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6waqp5oATjE"
   },
   "outputs": [],
   "source": [
    "# save to google drive \n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "\n",
    "!cp *.embs /gdrive/My\\ Drive/tooploox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MAUnLJeCZlaG"
   },
   "outputs": [],
   "source": [
    "!cp /gdrive/My\\ Drive/tooploox/*.embs ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UP891c-sYuQf"
   },
   "outputs": [],
   "source": [
    "!ls /gdrive/My\\ Drive/tooploox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMi42EReZye9"
   },
   "outputs": [],
   "source": [
    "features, target = load('resnet18_triplet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQL8T6YEkhS_"
   },
   "source": [
    "# Classification Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lpDtuaMfAUms"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "k_range = range(25,30)\n",
    "\n",
    "def search_knn_accuracies(k_range, features, targets):\n",
    "  acc = []\n",
    "  for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(features['train'], targets['train'])\n",
    "    #print('finished fitting')\n",
    "    predict = knn.predict(features['test'][:300,:])\n",
    "    #print('predicted')\n",
    "    score = metrics.accuracy_score(targets['test'][:300], predict)\n",
    "    print('K value: %d, accuracy: %0.7f' %(k, score))\n",
    "    acc.append(score)\n",
    "  return acc\n",
    "\n",
    "\n",
    "# the best score was obtained when k = 20:24\n",
    "acc = search_knn_accuracies(k_range, features, target)\n",
    "print('final Accuracy: ',sum(acc)/len(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aeQqnXTikqxO"
   },
   "source": [
    "# t-SNE plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFy1JSKSAZU8"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "RS = 123\n",
    "# need to create a subset of the data, too much time to process otherwise\n",
    "x_subset = features['train'][:2000]\n",
    "y_subset = targets['train'][:2000]\n",
    "\n",
    "print(np.unique(y_subset))\n",
    "labels = {\n",
    "     0: 'airplane',  \n",
    "     1: 'automobile',\n",
    "     2: 'bird',\n",
    "     3: 'cat',\n",
    "     4: 'deer',\n",
    "     5: 'dog',\n",
    "     6: 'frog',\n",
    "     7: 'horse',\n",
    "     8: 'ship',\n",
    "     9: 'truck',\n",
    "}\n",
    "# Utility function to visualize the outputs of PCA and t-SNE\n",
    "\n",
    "def fashion_scatter(x, colors):\n",
    "    # choose a color palette with seaborn.\n",
    "    num_classes = len(np.unique(colors))\n",
    "    palette = np.array(sns.color_palette(\"hls\", num_classes))\n",
    "\n",
    "    # create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # add the labels for each digit corresponding to the label\n",
    "    txts = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "\n",
    "        # Position of each label at median of data points.\n",
    "\n",
    "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(labels[i]), fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "\n",
    "    return f, ax, sc, txts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rD-BsELyA6BR"
   },
   "outputs": [],
   "source": [
    "# do pca before passing to tsne to reduce noice and fast performance\n",
    "time_start = time.time()\n",
    "\n",
    "pca_50 = PCA(n_components=50)\n",
    "pca_result_50 = pca_50.fit_transform(x_subset)\n",
    "\n",
    "print('PCA with 50 components done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "print('Cumulative variance explained by 50 principal components: {}'.format(np.sum(pca_50.explained_variance_ratio_)))\n",
    "# perform tsne on 50 components\n",
    "time_start = time.time()\n",
    "\n",
    "\n",
    "fashion_pca_tsne = TSNE(random_state=RS).fit_transform(pca_result_50)\n",
    "\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2HvhLmEA-ah"
   },
   "outputs": [],
   "source": [
    "fashion_scatter(fashion_pca_tsne, y_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fqfk3460dGYa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3T9eTgFHje3"
   },
   "outputs": [],
   "source": [
    "one = features['train'][0].reshape(1,-1)\n",
    "one_label = target['train'][0]\n",
    "print(one_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8y-f4Ss-an3J"
   },
   "outputs": [],
   "source": [
    "a = np.linalg.norm(features['test']-one, axis=1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BhTHY4xtat89"
   },
   "outputs": [],
   "source": [
    "new_a = np.append(a.reshape(-1,1), target['test'].reshape(-1,1), axis=1)\n",
    "new_a = new_a.tolist()\n",
    "new_a[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1EOcRCXJazOv"
   },
   "outputs": [],
   "source": [
    "def take_fist(elem):\n",
    "  return elem[0]\n",
    "\n",
    "sorted_a = sorted(new_a, key=take_fist)\n",
    "sorted_a = np.array(sorted_a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2rT2fYJa51f"
   },
   "outputs": [],
   "source": [
    "one_label_map = target['test'] == one_label\n",
    "sorted_a = one_label_map*sorted_a[:,0]\n",
    "sorted_a[:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LT5nU9SBa80D"
   },
   "outputs": [],
   "source": [
    "for i, num in enumerate(sorted_a):\n",
    "  print(i, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9tfzTz-bFoO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Training using Triplet loss.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
